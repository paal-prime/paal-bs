\chapter{Metrical TSP algorithms}


\section{Problem description}

\defproblem{ Metrical Travelling Salesman Problem}
{ Complete graph $G = (V,d)$, where $d:V \times V \to\mathbb{R}$ is a metric. }
{ What is the shortest cycle in $G$ which comes through every vertex exactly once? }

See link\footnote{\url{http://en.wikipedia.org/wiki/Travelling_salesman_problem}}
for detailed description.

\section{Christofides 1.5-apx}
Christofides algorithm\footnote{\url{http://en.wikipedia.org/wiki/Christofides_algorithm}} is designed to work with instances of TSP whose edge
weights satisfy triangle inequality. Let $G=(V, w)$ be an instance of TSP,
the algorithm consists of four steps:
\begin{enumerate}
\item Create minimum spanning tree $T$ for graph $G$. \\
Here we have implemented $\Oh(n^2)$ Prim's algorithm\footnote{\url{http://en.wikipedia.org/wiki/Prim's_algorithm}}.
Although for Euclidean spaces there exists faster $\Oh(n\log(n))$ algorithm, the overall gain
would be insignificant so we didn't use it.
\item Let $O$ be a set of odd degree vertices in $T$.
	  Find minimum perfect matching $M$ in complete graph over $O$. \\
For this part we have used \emph{Blossom V}\footnote{\url{http://pub.ist.ac.at/~vnk/software.html}}
implementation of Edmond's Blossom algorithm\footnote{\url{http://en.wikipedia.org/wiki/Blossom_algorithm}}.
Worst case running time of this part is $\Oh(|O|^5)$ but in average case it behaves very well.
\item Combine edges of $M$ and $T$ to form a multigraph $H$.
\item Find Eulerian circuit $E$ in $H$. ($\Oh(|H|)$).
\item Make $E$ Hamiltonian by shortcutting visited nodes. ($\Oh(|E|)$).
\end{enumerate}
Summing this up, our implementation in worst case needs $\Oh(|V|^5)$ time but generally
is fast in comparison to our other techniques.

\section{2opt Local Search}

In 2-opt\footnote{\url{http://en.wikipedia.org/wiki/2-opt}} local search for TSP, a single step
consists of reverting a segment of the current cycle. Intuitively, we find a point
at which the route crosses over itself and reorder it so that is doesn't.

Since in metrical TSP distance between a pair of points is the same for both directions,
computing fitness of the new route simplifies to replacing a pair of edges and replacing it with another
pair.

We've tested 3 different structures for maintaining the current local search solution:
\begin{itemize}
\item array of vertices in order as they appear on the cycle.
\item Reversible Segment List\footnote{\url{http://www.hars.us/Papers/revi.pdf}}
\item Augmented splay tree.
\end{itemize}

\begin{tabular}{c|cc}
& vertex read cost & reverse cost \\\hline
array & $\Oh(1)$ & $\Oh(|V|)$ \\
RSL & $\Oh(\sqrt{|V|})$ & $\Oh(\sqrt{|V|})$ amortized \\
splay & $\Oh(\log(|V|)$ amortized & $\Oh(\log(|V|))$ amortized
\end{tabular}

In our implementation segment to reverse is selected with uniform probability,
excluding the degenerated cases -- set of cycle edges has to actually change.
Evaluating fitness of neighbouring solution drawn requires to read vertices
on 4 positions in the cycle. The number of fitness evaluations dominates
the number of the actual reverses in the long run due to the fact that it
becomes harder and harder to find a neighbour with better fitness as we approach
the local optimum. As a consequence we have empirically observed that out of
3 data structure mentioned, the array performed best on TSPLIB (see Benchmarks)
test cases.

\section{Monte Carlo Tree Search}
We will refer to the concepts and definitions introduced in MCTS design
chapter. Let us remind that the only domain-dependent concepts in our MCTS
framework are Move and State and the former is completely dependent on the
latter.

It is widely known from games solving applications of MCTS method that we
should choose State implementation which minimizes branch factor of resulting
search tree. For TSP problem we chose path building approach:
\begin{itemize}
  \item State is a path with arbitrary start point (permutation of a subset of
    all vertices)
  \item initial State is an arbitrary chosen start vertex
  \item Move represents choosing from remaining vertices the next one to be
    added to the end of the path
  \item terminal State is reached when all vertices are in the path
  \item Fitness of terminal State is equal to cost of a Hamilton cycle, note
    that path defines permutation of vertices and distance function is a proper
    metric
  \item Fitness estimate is a fitness of terminal state reached after applying
    a sequence of random decisions
\end{itemize}

We have introduced two improvements to above schema. In order to reduce branch
factor of resulting tree we have limited number of vertices taken into
consideration when choosing next move. A good heuristics here is to choose from
no more than $K$ vertices closest to the path's end.

To improve accuracy of each sample we have decided to replace last $L$ random
moves with exhaustive search, therefore Fitness estimate is set to minimum of
fitnesses of all terminal states visited during this search. Typically $N$ is
very small, for implementation described above we have determined that the best
accuracy to performance ratio can be achieved when $N = 4$, it should be
obvious that $N$ depends on the State implementation rather than size of the
input graph. Note that this method works well with certain types of policies
and statistics aggregation methods as discussed in results overview.

\section{Benchmarks}
To evaluate the algorithms quality we have used
TSPLIB\footnote{\url{http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/}}
symmetrical instances.

\section{Results}

TODO

\subsection { 2-opt hill climb \& simulated annealing }

In this experiment we've shown that SA doesn't improve the results of the 2-opt.
Initial temperature has been set to mean edge cost of the initial solution.
Temperature falls geometrically over time reaching fixed final temperature at the last iteration.
Diagrams 4.1-3 present SA with different final temperatures ($10^{-2}, 10^{-5}, 10^{-9}$) and
pure hill climb strategy on sample TSPLIB test cases.

\input{../results/tsp/ls_convergence/brazil58}
\input{../results/tsp/ls_convergence/d15112}
\input{../results/tsp/ls_convergence/ulysses22}

First problem is that calculating temperature for boltzmann distribution is
expensive compared to a single iteration. We've dealt with it by updating temperature
only periodically. In our implementation mean SA iteration is about $30\%$ slower than
hill climb iteration. This fact hadn't been depicted on the diagrams in any way.

We can observe that hill climb converges strictly faster than any SA variant.
Moreover, the fitness curve is smooth (even though the neighbourhood is being browsed at random),
which means that not only 2-opt topology doesn't have many local optima but also that it has fitness
is "continuous" in it.

Basing on these observations, from this point we won't be considering 2-opt SA a
suitable solution for TSP.

\subsection{Monte Carlo Tree Search}

TODO % TODO pompon's tests

\subsubsection{Policies comparison}

Despite the poor performance of MCTS-based algorithms for TSP problem, we have
compared policies described in previous sections on small instances from
TSPLIB. The results are presented below. Each row corresponds to a single
policy (for names explanation please refer to the proper chapter). Each column
corresponds to the State representation being used. Column description has a
format \verb+<graph name>, limit <moves limit>+ where \emph{moves limit} is
the limit of the number of vertices taken into consideration when choosing next
move as described before. Number in each cell is equal to the final solution's
fitness.

The rest of configuration of each algorithm is exactly the same. In each test
run the algorithm preforms $500$ times the current number of vertices not in
the path samples per single decision.

\begin{figure}[ht]
  \centering
  \input{../results/tsp/mcts_policies_comparison/eil51}
\end{figure}

\begin{figure}[ht]
  \centering
  \input{../results/tsp/mcts_policies_comparison/eil76}
\end{figure}

\begin{figure}[ht]
  \centering
  \input{../results/tsp/mcts_policies_comparison/eil101}
\end{figure}

Results of this comparison were used in choosing the best algorithm's
configuration for other tests. We could also confirm a few observations made
during the course of fine-tunning our implementation.

Policies which choose the best node by minimizing expected fitness of a
solution outperform ones which minimize the lowest observed fitness and the
difference between them becomes smaller when the tree branch factor is being
reduced by restricting moves considered at each step. This can be heuristically
explained as a consequence of a performing exhaustive search when the size of
reachable solution's space is small enough (that means a few decisions before
reaching the terminal state). Once we are sure that after reaching a subtree
which is small enough we will find an optimal solution reachable from its root,
our objective is slightly different. Instead of looking for the best solution
we look for a subtree with pretty low fitness of reachable solutions in hope
that during the exhaustive search we will find better terminal state than we
have seen anytime before. We have made another observation which confirms above
conclusion. According to our tests the lowest fitness of any terminal state
reached during random sampling procedure is $5\% - 20\%$ higher than fitness of
the best state found during the final exhaustive search in the same run of an
algorithm.

Please note that the same pattern reproduces itself if one does not perform the
final exhaustive search. The main reason is that when the algorithm descends
into really small search subtree (say $6$ decisions before reaching the
terminal state) and the tree itself is highly uniform (estimates for each
children of a given node are roughly the same) all strategies reduce to picking
random path to some terminal state. When the number of samples per decision
exceeds the number of different terminal states the random sampling procedure
will visit all of them with high probability. The sole purpose of introduced
modification (the exhaustive search procedure) is to speed up entire process
and make sure that no matter how unlucky we are, we will always find the best
solution.

One can easily see that as far as the choice of the next node in the sampling
procedure is concerned there is no better strategy than uniform sampling. This
method is also the fastest one and its performance does not degrades when the
number of possible moves at each state (or a branch factor of the search tree)
increases. When one develops an algorithm which has a lot of magic constants
that affect its behaviour the problem of finding the best set of parameters
becomes nontrivial, uniform sampling is superior in this case as it guarantees
reasonable results without any fine tunning.

Note that the policies which prefer so-far-the best subtree during the random
sampling do not scale well when the branch factor of the search tree increases.
We suspect that these policies are completely useless for approximating
solution for instances with more than $100$ vertices as limiting the branch
factor greatly reduces reachable search space. Dramatically bad performance of
PolicyEpsBest can be easily explained given the previous observations.  This
policy not only seeks for a single best solution, which we already investigated
to be a bad idea, but also once a solution with low (but still far from
optimal) fitness is found in one subtree, the algorithm tends to visit this
subtree and greatly reduces chances of correcting this mistake.
